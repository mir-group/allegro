# general
root: results/aspirin
run_name: example
seed: 123456
dataset_seed: 123456
append: true
allow_tf32: false
default_dtype: float32

# -- network --
model_builders:
 - nequip_allegro.model.Allegro
 # the typical model builders from `nequip` can still be used:
 - PerSpeciesRescale
 - ForceOutput
 - RescaleEnergyEtc

# cutoffs
r_max: 6.0
avg_num_neighbors: auto
cutoff_type: polynomial  # `cosine` or `polynomial`

# radial basis
BesselBasis_trainable: true
PolynomialCutoff_p: 6  # sets it BOTH for the RadialBasisProjection AND the Allegro_Module
normalize_basis: true

# symmetry
l_max: 2
parity: o3_full  # allowed: o3_full, o3_restricted, so3

# Allegro layers:
num_layers: 3
env_embed_multiplicity: 32
embed_initial_edge: true
linear_after_env_embed: false

two_body_latent_mlp_latent_dimensions: [128, 256, 512, 1024]
two_body_latent_mlp_nonlinearity: silu
two_body_latent_mlp_initialization: uniform

latent_mlp_latent_dimensions: [1024]
latent_mlp_nonlinearity: silu
latent_mlp_initialization: uniform
latent_resnet: true

# Ratio of the update to the old latents in the resnet update:
# If not provided, defaults to all 1/2
latent_resnet_update_ratios: [0.5, 0.5, 0.5]

# Whether the ratios are trainble:
latent_resnet_update_ratios_learnable: false

env_embed_mlp_latent_dimensions: []
env_embed_mlp_nonlinearity: null
env_embed_mlp_initialization: uniform

# - end allegro layers -

# Final MLP to go from Allegro latent space to edge energies:
edge_eng_mlp_latent_dimensions: [128]
edge_eng_mlp_nonlinearity: null
edge_eng_mlp_initialization: uniform

# Learnable per species pair scale to the _edgewise_ energy -
# can be combined with a per species scale
# As in, C-C gets one scale, C-H gets another, H-C another, etc.
per_edge_species_scale: false

# -- data --
dataset: npz                                                                       # type of data set, can be npz or ase
dataset_url: http://quantum-machine.org/gdml/data/npz/aspirin_ccsd.zip             # url to download the npz. optional
dataset_file_name: ./benchmark_data/aspirin_ccsd-train.npz                         # path to data set file
key_mapping:
  z: atomic_numbers                                                                # atomic species, integers
  E: total_energy                                                                  # total potential eneriges to train to
  F: forces                                                                        # atomic forces to train to
  R: pos                                                                           # raw atomic positions
npz_fixed_field_keys:                                                              # fields that are repeated across different examples
  - atomic_numbers

# A mapping of chemical species to type indexes is necessary if the dataset is provided with atomic numbers instead of type indexes.
chemical_symbol_to_type:
  H: 0
  C: 1
  O: 2

# logging
wandb: false
wandb_project: aspirin
verbose: debug

# training
n_train: 950
n_val: 50
batch_size: 5
max_epochs: 1000000
learning_rate: 0.002
train_val_split: random
shuffle: true
metrics_key: validation_loss

# use an exponential moving average of the weights
use_ema: true
ema_decay: 0.99
ema_use_num_updates: true

# loss function
loss_coeffs:
  forces: 1.
  total_energy:
    - 1.
    - PerAtomMSELoss

# optimizer
optimizer_name: Adam
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.

# lr scheduler, drop lr if no improvement for 50 epochs
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 50
lr_scheduler_factor: 0.5

# early stopping if max 7 days is reached or lr drops below 1e-5 or no improvement on val loss for 100 epochs
early_stopping_upper_bounds:
  cumulative_wall: 604800.

early_stopping_lower_bounds:
  LR: 1.0e-5

early_stopping_patiences:
  validation_loss: 100

