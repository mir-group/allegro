run: [train, test]

cutoff_radius: 5.0
chemical_symbols: [C, O, H]
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.sGDML_CCSD_DataModule
  dataset: aspirin
  data_source_dir: aspirin_data
  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}
  trainval_test_subset: [40, 10]
  train_val_split: [30, 10]
  seed: 123
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 1
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 5
  test_dataloader: ${data.val_dataloader}
  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  max_epochs: 5
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}
      save_last: true


# NOTE:
# interpolation parameters for Allegro model
num_scalar_features: 64


training_module:
  _target_: nequip.train.EMALightningModule
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 1.0
      forces: 1.0
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      per_atom_energy_mae: 1.0
      forces_mae: 1.0
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
  # ^ IMPORTANT: Allegro models do better with learning rates around 1e-3

  # to use the Allegro model in the NequIP framework, the following `model` block has to be changed to be that of Allegro's
  # see Allegro model docs for explanation of model hyperparameters:
  # https://nequip.readthedocs.io/projects/allegro/en/latest/guide/allegro_model.html
  model:
    _target_: allegro.model.AllegroModel

    # If you have PyTorch >= 2.6.0 installed, and are training on GPUs, the following line uses torch.compile to speed up training
    # for more details, see https://nequip.readthedocs.io/en/latest/guide/accelerations/pt2_compilation.html
    compile_mode: compile
    # ^ if you're using PyTorch <= 2.6.0, an error will be thrown -- comment out the line to avoid it

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # === two-body scalar embedding ===
    radial_chemical_embed:
      # the defaults for the Bessel embedding module are usually appropriate
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6

    # output dimension of the radial-chemical embedding
    radial_chemical_embed_dim: ${num_scalar_features}

    # scalar embedding MLP
    scalar_embed_mlp_hidden_layers_depth: 1
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # === core hyperparameters ===
    # The following hyperparameters are the main ones that one should focus on tuning.

    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 1

    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 2

    # number of scalar features, more is more accurate but slower
    # 16, 32, 64, 128, 256 are good options to try depending on the dataset
    num_scalar_features: ${num_scalar_features}

    # number of tensor features, more is more accurate but slower
    # 8, 16, 32, 64 are good options to try depending on the dataset
    num_tensor_features: 32

    # == allegro MLPs ==
    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layers_depth: 1
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    # ^ setting `nonlinearity` to `null` means that the Allegro MLPs are effectively linear layers

    # === advanced hyperparameters ===
    # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

    # whether to include features with odd mirror parity
    # often turning parity off gives equally good results but faster networks, so do consider this
    parity: true

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is `true`, which is expected to be more expressive than `false`
    tp_path_channel_coupling: true

    # == readout MLP ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    # ^ setting `nonlinearity` to `null` means that output MLP is effectively a linear layer

    # === misc hyperparameters ===
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # per-type per-atom scales and shifts
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    # ^ IMPORTANT: it is usually useful and important to use isolated atom energies computed with the same method used to generate the training data
    #   they should be provided as a dict, e.g.
    # per_type_energy_shifts:
    #   C: 1.234
    #   H: 2.345
    #   O: 3.456
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # ZBL pair potential (optional, can be removed or included depending on aplication)
    # see NequIP docs for details:
    # https://nequip.readthedocs.io/en/latest/api/nn.html#nequip.nn.pair_potential.ZBL
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: real     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types


global_options:
  allow_tf32: false
